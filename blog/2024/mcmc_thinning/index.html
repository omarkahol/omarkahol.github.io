<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> MCMC thinning | Omar Kahol </title> <meta name="author" content="Omar Kahol"> <meta name="description" content="An analysis of the counter-benefits that arise from thinning or sub-sampling for MCMC"> <meta name="keywords" content="Electrohydrodynamics, Corona Thruster, Aircraft icing, Uncertainty Quantification, Bayesian Methods"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://omarkahol.github.io/blog/2024/mcmc_thinning/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Omar </span> Kahol </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">MCMC thinning</h1> <p class="post-meta"> February 22, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/category/statistics"> <i class="fa-solid fa-tag fa-sm"></i> Statistics</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>The following blog post analyses the effect of subsampling a Markov chain. Subsampling (or thinning) is a common operation that consists of discarding N samples before saving the next one when sampling an MCMC (Markov Chain Monte Carlo). This is done for two main reasons. First and foremost, it saves memory by reducing the number of samples that have to be analyzed. Moreover; even with a perfect acceptance rate, samples will always be correlated. To ensure independence between samples, they have to be “thinned”; between two saved samples, some samples compatible with the correlation length need to be discarded. Such samples will be then used to construct statistics or estimators so it is natural to ask if subsampling is necessary. If the answer is yes, how much? We will try to answer this question by looking at the quality of the mean estimator as a function of the subsampling ratio.</p> <hr> \[\newcommand{\muh}{\hat{\mu}} \newcommand{\summ}[3]{\displaystyle \sum_{#1=#2}^{#3}} \newcommand{\dd}{\text{d}} \newcommand{\qt}{\tilde{q}} \newcommand{\var}{\text{var}} \newcommand{\cov}{\text{cov}} \newcommand{\vh}{\hat{V}}\] <p>Assume that we have \(n\) samples \(\{q_i\}_{i&lt;n}\) taken from the realization of a Markov chain that has converged to the stationary distribution \(p(q)\). The expected value of the distribution will be estimated using the sample average in the following way</p> \[\begin{equation} \left. \begin{aligned} &amp; \mu = E(q) = \int_\mathcal{Q} q \ p(q) \, \dd q \; , \\ &amp; \muh = \dfrac{1}{n} \summ{i}{1}{n} q_i \; . \end{aligned} \right. \end{equation}\] <p>Even if the samples are not independent, the expected value of this estimator will be, because of the linearity of the expectation operator:</p> \[\begin{equation} E(\muh) = \dfrac{1}{n}\summ{i}{1}{n} E(q_i) = \mu \; . \end{equation}\] <p>Defining \(\qt_i = q_i - \mu\), we can compute the variance in the following way:</p> \[\begin{equation} \var(\muh) = E\left( (\muh -\mu)^2\right) = E\left( \dfrac{1}{n^2} \summ{i}{1}{n} \qt_i \summ{j}{1}{n} \qt_j\right) \; . \end{equation}\] <p>The product between the sample sum can be decomposed by noting that</p> \[\begin{equation} \left. \begin{aligned} (\qt_1 + \qt_2 + \ldots+ \qt_n)&amp; (\qt_1 + \qt_2 + \ldots + \qt_n)= \\ &amp; \qt_1^2 + \qt_2^2 + \ldots + \qt_n^2 \\ &amp; +2\qt_1 \qt_2 + \ldots + 2\qt_1 \qt_n \\ &amp;+ \ldots + 2 \qt_2 \qt_n \\ &amp; + \ldots \end{aligned} \right. \end{equation}\] <p>Hence,</p> \[\begin{equation} \var(\muh) = \dfrac{1}{n^2} E\left( \summ{i}{1}{n} \qt_i^2 + 2 \summ{i}{1}{n-1}\summ{j}{i+1}{n} \qt_i\qt_j\right) = \dfrac{1}{n^2} \left( \summ{i}{1}{n} E(\qt_i^2) + 2 \summ{i}{1}{n-1}\summ{j}{i+1}{n} E(\qt_i\qt_j) \right) \; . \end{equation}\] <p>And finally,</p> \[\begin{equation} \var(\muh) = \dfrac{\sigma^2}{n} + \dfrac{2}{n^2} \summ{i}{1}{n-1}\summ{j}{i+1}{n} \cov(\qt_i,\qt_j) \; . \label{eq:var} \end{equation}\] <p>We now assume an exponential decay of the covariance between samples,</p> \[\begin{equation} \cov(\qt_i,\qt_j) = \sigma^2 \ \exp\left( -\dfrac{|i-j|}{L}\right) \; , \label{eq:cov} \end{equation}\] <p>where the factor \(\sigma^2\) ensures that when \(i=j\) we retrieve the correct variance and \(L\) is an appropriate correlation length scale. We denote \(k=\mid i-j\mid\) as the lag. Substituting Eq. \ref{eq:cov} into Eq. \ref{eq:var} yields</p> \[\begin{equation} \left. \begin{aligned} \var(\muh) &amp; = \dfrac{\sigma^2}{n} + \dfrac{2}{n^2} \summ{i}{1}{n-1}\summ{k}{1}{n} \exp\left( -\dfrac{k}{L}\right) \\ &amp; = \dfrac{\sigma^2}{n} \left( 1+ 2\frac{n-1}{n} \summ{k}{1}{n} \exp\left( -\dfrac{k}{L}\right) \right)\; . \end{aligned} \right. \end{equation}\] <p>To rewrite the last term we have used the fact that the term inside the exponential does not depend on the running index i. Now we introduce the thinning parameter s which can be an integer value between 1 and n-1. It corresponds to how much the chain is thinned, i.e. we retain 1 every s samples. As an example, when s is equal to 1 we do not discard any sample and we immediately save the next sample. When s is equal to n-1 we accept the first sample and then discard all the subsequent n-1 samples, remaining with just one sample. For the sake of brevity, we also call \(\tilde{n}=\frac{n/s-1}{n/s}\)</p> \[\begin{equation} \var(\muh) = \dfrac{\sigma^2}{n/s} \left( 1+ 2 \tilde{n}\summ{k}{1}{n/s} \exp\left( -\dfrac{k s}{L}\right) \right)\; . \end{equation}\] <p>The sum evaluates to,</p> \[\begin{equation} \summ{k}{1}{n/s} \exp\left( -\dfrac{k s}{L}\right) = \dfrac{e^{-n/L}\left( e^{n/L}-1\right)}{e^{s/L}-1} \; . \end{equation}\] <p>We now compute the ratio between the sample variance and \(L\frac{\sigma^2}{n}\) which has the dimensionality of variance and does not depend on the thinning parameter, \(s\),</p> \[\begin{equation} \dfrac{\var(\muh)}{L \frac{\sigma^2}{n}} = \frac{s}{L} \left( 1 - 2 \tilde{n} \dfrac{1-e^{-n/L}}{1-e^{s/L}} \right)\; . \end{equation}\] <p>We now proceed by looking at the last equation as a function of \(x=s/L\) (which compares the amount of skipping to the correlation length), we call \(c = \tilde{n} \left(1-e^{-n/L} \right)\),</p> \[\begin{equation} f(x) = x \left(1 - 2\dfrac{c}{1-e^x} \right) \; . \end{equation}\] <p>The parameter c is limited by 0 when \(n \rightarrow 0\) or \(L \rightarrow \infty\) and 1 when \(n \rightarrow \infty\) or \(L \rightarrow 0\) (assuming that we still accept a sufficient number of samples so that \(\tilde{n}=1\), otherwise we get a lower value). We call these limits the fully correlated and uncorrelated limits, respectively. We can show that this function is always monotone (for values of c in the range [0,1]). This means that thinning has no beneficial effect on the quality of the estimator. The following graph shows the increase in variance caused by thinning</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/mcmc_thinning/corr1-480.webp 480w,/assets/img/posts/mcmc_thinning/corr1-800.webp 800w,/assets/img/posts/mcmc_thinning/corr1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/mcmc_thinning/corr1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="correlation" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>On the one hand, subsampling always has a detrimental effect on the quality of the estimator. On the other hand, it is possible to see that this effect is negligible up to \(s/L \approx 1\), meaning that we can still subsample without any negative effect and reduce the dimension of the analyzed samples.<br> This has a positive effect as we have to analyze and store fewer samples.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Omar Kahol. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>